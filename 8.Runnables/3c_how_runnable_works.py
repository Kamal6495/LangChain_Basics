# --------------------------
# Step 1: Define Runnable Abstract Base Class
# --------------------------

from abc import ABC, abstractmethod
import random


class Runnable(ABC):
    """Abstract base class that enforces a common interface for all components.

    Every subclass must implement the `invoke` method. This ensures that all
    components can be chained together in a standardized way.
    """

    @abstractmethod
    def invoke(self, input_data):
        """Process the given input and return an output.

        Args:
            input_data: Any input required by the component.

        Returns:
            Any output generated by the component.
        """
        pass


# --------------------------
# Component 1: Dummy LLM
# --------------------------

class NakliLLM(Runnable):
    """A dummy Language Model simulator.

    This class mimics the behavior of an LLM by randomly selecting a response
    from a predefined list.
    """

    def __init__(self):
        print('LLM Created')

    def invoke(self, prompt):
        """Simulate model inference.

        Args:
            prompt (str): The input prompt string.

        Returns:
            dict: A dictionary with key 'response' containing model output.
        """
        response_list = [
            'Lucknow Capital of UP',
            'T20 World Cup 2024 India',
            'Physics Based Neural Network'
        ]
        return {'response': random.choice(response_list)}

    def predict(self, prompt):
        """Legacy method (to be deprecated).

        Args:
            prompt (str): The input prompt string.

        Returns:
            dict: Similar to `invoke` but appends a warning note.
        """
        response_list = [
            'Lucknow Capital of UP',
            'T20 World Cup 2024 India',
            'Physics Based Neural Network'
        ]
        return {
            'response': random.choice(response_list) +
                        '\n***Warning: This Method will be Deprecated***'
        }


# --------------------------
# Component 2: Dummy Prompt Template
# --------------------------

class NakliPromptTemplate(Runnable):
    """A simple prompt templating class.

    Takes a string template with placeholders and fills it using input variables.
    """

    def __init__(self, template, input_variables):
        """
        Args:
            template (str): Template string with placeholders (e.g. "Hello {name}")
            input_variables (list[str]): List of variables expected in the template.
        """
        self.template = template
        self.input_variables = input_variables

    def invoke(self, input_dict):
        """Format the template with actual values.

        Args:
            input_dict (dict): Dictionary mapping placeholders to values.

        Returns:
            str: The formatted string.
        """
        return self.template.format(**input_dict)

    def format(self, input_dict):
        """Alias for invoke(), kept for flexibility."""
        return self.template.format(**input_dict)


# --------------------------
# Component 3: Output Parser
# --------------------------

class NakliOutputParser(Runnable):
    """Parser to extract response value from a dictionary."""

    def __init__(self):
        pass

    def invoke(self, input_data):
        """Extract only the 'response' field from input data.

        Args:
            input_data (dict): Dictionary containing a 'response' key.

        Returns:
            str: Extracted response text.
        """
        return input_data['response']


# --------------------------
# Component 4: Combine LLM and Prompt
# --------------------------

class NakliLLMChain(Runnable):
    """A chain that combines a prompt template with an LLM.

    It first formats the prompt, then sends it to the LLM, and returns the output.
    """

    def __init__(self, llm, prompt):
        """
        Args:
            llm (Runnable): LLM-like component.
            prompt (Runnable): Prompt template component.
        """
        self.llm = llm
        self.prompt = prompt

    def invoke(self, input_dict):
        """Run the chain using standardized invoke() method.

        Args:
            input_dict (dict): Input data for prompt formatting.

        Returns:
            str: Final response text from the LLM.
        """
        final_prompt = self.prompt.invoke(input_dict)
        print("Final Prompt:", final_prompt)
        result = self.llm.invoke(final_prompt)
        return result['response']

    def run(self, input_dict):
        """Run the chain using legacy predict() of LLM.

        Args:
            input_dict (dict): Input data for prompt formatting.

        Returns:
            str: Response text with a warning note.
        """
        final_prompt = self.prompt.format(input_dict)
        print("Final Prompt:", final_prompt)
        result = self.llm.predict(final_prompt)
        return result['response']


# --------------------------
# Component 5: Generalized Runnable Connector
# --------------------------

class RunnableConnector(Runnable):
    """A generalized pipeline that connects multiple runnables.

    It sequentially passes output of one component as input to the next.
    """

    def __init__(self, runnable_list):
        """
        Args:
            runnable_list (list[Runnable]): List of components to chain together.
        """
        self.runnable_list = runnable_list

    def invoke(self, input_data):
        """Execute the pipeline.

        Args:
            input_data: Initial input for the first component.

        Returns:
            Final output after passing through all runnables.
        """
        for runnable in self.runnable_list:
            input_data = runnable.invoke(input_data)
        return input_data


# --------------------------
# Example Usage For Runnable Connector (Generalized)
# --------------------------

# Create Prompt Template
template = NakliPromptTemplate(
    template="Write a short note on India's {topic}",
    input_variables=['topic']
)

# Create Fake LLM
llm = NakliLLM()

# Create Parser
parser = NakliOutputParser()

# Build Chain: [PromptTemplate → LLM → OutputParser]
chain = RunnableConnector([template, llm, parser])

# Run with input
result = chain.invoke({'topic': 'Peacock'})
print(result)


# --------------------------
# Example Usage for NakliLLMChain (Speialized)
# --------------------------

template = NakliPromptTemplate(
    template="Write a short note on India's {topic}",
    input_variables=['topic']
)
llm = NakliLLM()
llm_chain = NakliLLMChain(llm, template)

# Run chain with standardized invoke()
result1 = llm_chain.invoke({'topic': 'Peacock'})
print("Output using invoke():", result1)

# Run chain with alternate run() method
result2 = llm_chain.run({'topic': 'Peacock'})
print("Output using run():", result2)
